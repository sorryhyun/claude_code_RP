"""
Agent response generation for multi-agent conversations.

This module handles the logic for generating individual agent responses,
including context building, API calls, and message broadcasting.
"""

import logging
import os
import time
from datetime import datetime
from typing import Optional

import crud
import schemas
from config.constants import SKIP_MESSAGE_TEXT
from domain.contexts import AgentMessageData, AgentResponseContext, ImageData, MessageContext, OrchestrationContext
from services.memory_mode_service import get_memory_mode_service
from utils.conversation_utils import detect_conversation_type
from utils.helpers import get_pool_key
from utils.timezone import format_kst_timestamp

from .context import build_conversation_context
from .handlers import (
    broadcast_stream_delta,
    broadcast_stream_end,
    broadcast_stream_start,
    broadcast_typing_indicator,
)

logger = logging.getLogger("ResponseGenerator")


def save_critic_report(agent_name: str, response_text: str, thinking_text: str = ""):
    """
    Save a critic agent's diagnostic report to reports/report.md

    Args:
        agent_name: Name of the critic agent
        response_text: The diagnostic report content
        thinking_text: Optional thinking process
    """
    try:
        # Create reports directory if it doesn't exist
        reports_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "reports")
        os.makedirs(reports_dir, exist_ok=True)

        # Create timestamped report file path to avoid overwriting
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Sanitize agent name for filename (replace spaces and special chars with underscores)
        safe_agent_name = "".join(c if c.isalnum() else "_" for c in agent_name)
        report_filename = f"{safe_agent_name}_{timestamp}.md"
        report_path = os.path.join(reports_dir, report_filename)

        # Format report with timestamp and metadata
        readable_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_content = f"""# Diagnostic Report
**Generated by:** {agent_name}
**Timestamp:** {readable_timestamp}

---

{response_text}
"""

        # Optionally append thinking process if available
        if thinking_text:
            report_content += f"\n\n---\n\n## Thinking Process\n\n{thinking_text}\n"

        # Write to file
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        logger.info(f"üìù Critic report saved to {report_path}")

    except Exception as e:
        logger.error(f"‚ùå Failed to save critic report: {e}")


class ResponseGenerator:
    """
    Handles the generation of responses from individual agents.

    This class is responsible for:
    - Building conversation context for each agent
    - Calling memory-brain to surface relevant long-term memories
    - Generating agent responses via AgentManager
    - Handling interruption checks
    - Broadcasting responses to clients
    """

    def __init__(self, last_user_message_time: dict[int, float], memory_brain):
        """
        Initialize the response generator.

        Args:
            last_user_message_time: Shared dict tracking last user message timestamp per room
            memory_brain: MemoryBrain instance for selecting memories
        """
        self.last_user_message_time = last_user_message_time
        self.memory_brain = memory_brain

    async def generate_response(
        self,
        orch_context: OrchestrationContext,
        agent,
        user_message_content: Optional[str] = None,
        is_critic: bool = False,
    ) -> bool:
        """
        Generate a response from a single agent.
        In initial responses, this runs concurrently with other agents.
        In follow-up rounds, this runs sequentially.

        Args:
            orch_context: OrchestrationContext containing db, room_id, manager, agent_manager
            agent: Agent object
            user_message_content: The user's message (for initial responses), or None for follow-ups
            is_critic: If True, stores feedback but doesn't broadcast to main chat

        Returns:
            True if agent responded, False if agent skipped
        """
        # Record when this response generation started
        # Used to check if it was interrupted by a new user message
        response_start_time = time.time()

        # Generate unique task ID for interruption tracking
        task_id = get_pool_key(orch_context.room_id, agent.id)

        # Fetch room to get created_at timestamp (use cache for performance)
        room = await crud.get_room_cached(orch_context.db, orch_context.room_id)

        # Fetch only the messages since this agent's last response (cache for performance)
        room_messages = await crud.get_messages_after_agent_response_cached(
            orch_context.db,
            orch_context.room_id,
            agent.id,
            limit=120,
        )

        # Get agent config data
        agent_config = agent.get_config_data()

        # Get number of agents in the room
        agent_count = len(room.agents) if room else 0

        # Determine conversation type and participants using shared utility
        _, user_name, has_situation_builder = detect_conversation_type(room_messages, agent_count)

        # Build conversation context from room messages (only new messages since agent's last response)
        conversation_context = build_conversation_context(
            room_messages,
            limit=25,
            agent_id=agent.id,
            agent_name=agent.name,
            agent_count=agent_count,
            user_name=user_name,
        )

        # For follow-up rounds, skip if there are no new messages since this agent's last response
        if user_message_content is None:
            if not self._has_new_messages(conversation_context):
                return False

        # Create message context for handlers
        msg_context = MessageContext(db=orch_context.db, room_id=orch_context.room_id, agent=agent)

        # Get this agent's session for this specific room
        session_id = await crud.get_room_agent_session(orch_context.db, orch_context.room_id, agent.id)

        # Use conversation context which already includes all messages and instructions
        message_to_agent = conversation_context if conversation_context else "Start the conversation."

        # Format conversation start timestamp
        conversation_started = None
        if room and room.created_at:
            conversation_started = format_kst_timestamp(room.created_at, "%Y-%m-%d %H:%M:%S KST")

        # Memory-brain: Surface relevant long-term memories before generating response
        memory_injection = await self._inject_memories_if_enabled(
            agent=agent,
            agent_config=agent_config,
            room_messages=room_messages,
            agent_count=agent_count,
            user_name=user_name,
            has_situation_builder=has_situation_builder,
            msg_context=msg_context,
            is_critic=is_critic,
        )

        # Inject memories into message if available
        if memory_injection:
            message_to_agent = memory_injection + message_to_agent

        # Check if the most recent user message has an image attachment
        # Only pass image for initial responses (user_message_content is not None)
        image_data = None
        if user_message_content is not None and room_messages:
            # Find the most recent user message with an image
            for msg in reversed(room_messages):
                if msg.role == "user" and hasattr(msg, "image_data") and msg.image_data:
                    import json

                    try:
                        # Parse image_data from JSON string if needed
                        img_dict = json.loads(msg.image_data) if isinstance(msg.image_data, str) else msg.image_data
                        if img_dict and isinstance(img_dict, dict):
                            image_data = ImageData(
                                data=img_dict.get("data", ""),
                                media_type=img_dict.get("media_type", "image/png"),
                            )
                            logger.info(f"üì∑ Found image attachment in user message for agent: '{agent.name}'")
                    except (json.JSONDecodeError, TypeError) as e:
                        logger.warning(f"Failed to parse image_data: {e}")
                    break  # Only use the most recent image

        # Build agent response context
        logger.debug(f"Building response context for agent: '{agent.name}' (id: {agent.id})")
        response_context = AgentResponseContext(
            system_prompt=agent.system_prompt,
            user_message=message_to_agent,
            agent_name=agent.name,
            config=agent.get_config_data(),
            room_id=orch_context.room_id,
            agent_id=agent.id,
            session_id=session_id,
            conversation_history=None,  # Not needed - already in message_to_agent
            task_id=task_id,
            conversation_started=conversation_started,
            has_situation_builder=has_situation_builder,
            image_data=image_data,
        )

        # Handle streaming response events
        temp_id = None
        response_text = ""
        thinking_text = ""
        new_session_id = session_id
        memory_entries = []
        skipped = False
        stream_started = False

        # Iterate over streaming events from agent manager
        async for event in orch_context.agent_manager.generate_sdk_response(response_context):
            event_type = event.get("type")

            if event_type == "stream_start":
                temp_id = event.get("temp_id")
                # Broadcast stream_start (skip for critics)
                if not is_critic:
                    await broadcast_stream_start(msg_context, temp_id)
                stream_started = True

            elif event_type == "content_delta":
                content_delta = event.get("delta", "")
                response_text += content_delta
                # Broadcast content delta (skip for critics)
                if not is_critic:
                    await broadcast_stream_delta(msg_context, temp_id, content_delta=content_delta)

            elif event_type == "thinking_delta":
                thinking_delta = event.get("delta", "")
                thinking_text += thinking_delta
                # Broadcast thinking delta (skip for critics)
                if not is_critic:
                    await broadcast_stream_delta(msg_context, temp_id, thinking_delta=thinking_delta)

            elif event_type == "stream_end":
                # Extract final data
                response_text = event.get("response_text") or response_text
                thinking_text = event.get("thinking_text") or thinking_text
                new_session_id = event.get("session_id", session_id)
                memory_entries = event.get("memory_entries", [])
                skipped = event.get("skipped", False)

        # Memory entries are now written directly by the memorize tool
        # So we can skip this section (kept for reference/debugging)
        if memory_entries:
            logger.debug(
                f"üìù Agent {agent.name} recorded {len(memory_entries)} memories (handled by memorize tool directly)"
            )

        # Update this room-agent session_id if it changed
        if new_session_id and new_session_id != session_id:
            await crud.update_room_agent_session(orch_context.db, orch_context.room_id, agent.id, new_session_id)

        # Skip if agent chose not to respond
        if skipped or not response_text:
            # Save and broadcast skip message if stream was started (so frontend can show persistent skip indicator)
            if stream_started and temp_id and not is_critic:
                # Save skip message to database
                skip_message = schemas.MessageCreate(
                    content=SKIP_MESSAGE_TEXT,
                    role="assistant",
                    agent_id=agent.id,
                    thinking=thinking_text if thinking_text else None,
                )
                # Don't update room activity for skip messages
                saved_skip_msg = await crud.create_message(
                    orch_context.db, orch_context.room_id, skip_message, update_room_activity=False
                )

                # Broadcast skip event with database ID (if agent_manager is available)
                if orch_context.agent_manager:
                    from utils.timezone import make_timezone_aware

                    timestamp_aware = make_timezone_aware(saved_skip_msg.timestamp)
                    await orch_context.agent_manager.broadcast(
                        {
                            "type": "stream_skip",
                            "temp_id": temp_id,
                            "id": saved_skip_msg.id,
                            "timestamp": timestamp_aware.isoformat(),
                        },
                        orch_context.room_id,
                    )
            return False

        # Check if this response was interrupted by a new user message
        # If a user message arrived after this response started, skip broadcasting it
        if self._was_interrupted(orch_context.room_id, response_start_time, agent.name):
            return False

        # Check if room was paused while this agent was generating
        # This prevents messages from being saved after pause button is pressed
        if room and room.is_paused:
            logger.info(f"‚è∏Ô∏è  Room {orch_context.room_id} was paused. Discarding response from {agent.name}")
            return False

        # For critic agents, automatically save their output to report.md
        if is_critic:
            save_critic_report(agent.name, response_text, thinking_text)

        # Save message to database and broadcast stream_end (critics: save only, don't broadcast)
        message_data = AgentMessageData(content=response_text, thinking=thinking_text)
        await broadcast_stream_end(msg_context, temp_id, message_data, broadcast=not is_critic)

        return True

    async def _inject_memories_if_enabled(
        self,
        agent,
        agent_config,
        room_messages: list,
        agent_count: int,
        user_name: str,
        has_situation_builder: bool,
        msg_context: MessageContext,
        is_critic: bool,
    ) -> str:
        """
        Surface and inject relevant long-term memories using memory-brain.

        Only active when MEMORY_MODE=BRAIN (global override) and agent has memory-brain enabled.

        Args:
            agent: Agent object
            agent_config: Agent configuration data
            room_messages: Recent room messages for context
            agent_count: Number of agents in the room
            user_name: User's display name
            has_situation_builder: Whether room has situation builder
            msg_context: Message context for broadcasting
            is_critic: If True, skip typing indicator broadcast

        Returns:
            Memory injection text to prepend to agent message, or empty string
        """
        # Check if memory-brain is enabled for this agent
        memory_service = get_memory_mode_service()
        if not (
            memory_service.is_memory_brain_enabled(agent_config.memory_brain_enabled)
            and agent_config.long_term_memory_index
        ):
            return ""

        logger.info(
            f"üß† [BRAIN MODE] Memory-brain enabled for {agent.name} with policy: {agent_config.memory_brain_policy}"
        )

        # Broadcast typing indicator to show memory brain is working (only if not critic)
        if not is_critic:
            await broadcast_typing_indicator(msg_context)
            logger.debug("üì° Broadcasting typing indicator for memory brain analysis")

        # Build list of available memory entries
        from domain.memory import MemoryEntry

        available_memories = []
        for mem_id, mem_content in agent_config.long_term_memory_index.items():
            # Create preview (first 100 chars)
            preview = mem_content[:100] + "..." if len(mem_content) > 100 else mem_content
            available_memories.append(
                MemoryEntry(
                    id=mem_id,
                    tags=[],  # Tags could be extracted from markdown headers if needed
                    content_preview=preview,
                )
            )

        # Build clean conversation context for memory brain (without response instruction)
        memory_brain_context = build_conversation_context(
            room_messages,
            limit=25,
            agent_id=agent.id,
            agent_name=None,  # No agent name needed for memory brain
            agent_count=agent_count,
            user_name=None,  # No user name needed for memory brain
            include_response_instruction=False,  # Memory brain doesn't need response instruction
        )

        # Call memory-brain to select memories
        # The brain will naturally avoid redundancy by seeing the conversation context
        try:
            memory_result = await self.memory_brain.analyze(
                conversation_context=memory_brain_context,
                available_memories=available_memories,
                policy=agent_config.memory_brain_policy,
                agent_name=agent.name,
                in_a_nutshell=agent_config.in_a_nutshell or "",
                characteristics=agent_config.characteristics or "",
                agent_count=agent_count,
                user_name=user_name,
                has_situation_builder=has_situation_builder,
            )

            # If memories should be injected, build injection text
            if memory_result.should_inject_now and memory_result.activated_memories:
                logger.info(f"üí° Injecting {len(memory_result.activated_memories)} memories for {agent.name}")

                memory_texts = []
                for activation in memory_result.activated_memories:
                    # Look up full memory content
                    full_content = agent_config.long_term_memory_index.get(activation.memory_id, "")
                    if full_content:
                        # Summarize memory if too long (keep first 300 chars)
                        summary = full_content[:300] + "..." if len(full_content) > 300 else full_content
                        memory_texts.append(
                            f"**Memory: {activation.memory_id}** (strength: {activation.activation_strength:.1f})\n{summary}"
                        )

                if memory_texts:
                    return "\n\n---\n**Surfaced Memories:**\n" + "\n\n".join(memory_texts) + "\n---\n\n"

            logger.debug(f"No memories surfaced for {agent.name} (should_inject={memory_result.should_inject_now})")

        except Exception as e:
            logger.error(f"‚ùå Memory-brain error for {agent.name}: {e}")
            # Continue without memory injection on error

        return ""

    def _has_new_messages(self, conversation_context: str) -> bool:
        """
        Check if conversation_context contains new messages for the agent to respond to.

        Args:
            conversation_context: The formatted conversation context

        Returns:
            True if there are new messages, False otherwise
        """
        # Check if conversation_context is empty or only contains the header/footer
        # The context builder adds a header and footer, so minimal context has ~2 lines
        context_lines = conversation_context.strip().split("\n")

        # Filter out header/footer lines
        actual_messages = [
            line
            for line in context_lines
            if line
            and not line.startswith("Here's the recent conversation")
            and not line.startswith("Respond naturally")
        ]

        return bool(actual_messages)

    def _was_interrupted(self, room_id: int, response_start_time: float, agent_name: str) -> bool:
        """
        Check if this response was interrupted by a new user message.

        Args:
            room_id: Room ID
            response_start_time: When this response generation started
            agent_name: Name of the agent (for logging)

        Returns:
            True if interrupted, False otherwise
        """
        if room_id in self.last_user_message_time:
            last_user_msg_time = self.last_user_message_time[room_id]
            if last_user_msg_time > response_start_time:
                logger.info(
                    f"‚è≠Ô∏è  SKIPPING BROADCAST | Room: {room_id} | Agent: {agent_name} | "
                    f"Response started at {response_start_time:.3f}, but interrupted by user message at {last_user_msg_time:.3f}"
                )
                return True
        return False
