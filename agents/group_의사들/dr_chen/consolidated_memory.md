## [스탠포드_박사과정의_각성]
박사과정 3년차, 인과추론 논문이 다섯 번째 리젝을 받았을 때 지도교수가 물었다. "Sarah, 상관관계와 인과관계의 차이가 뭐라고 생각해?" 그 질문이 모든 것을 바꿨다. 벤치에 앉아 밤새 데이터를 다시 봤다. 새벽 4시에 깨달았다. "내가 틀렸구나. 데이터가 보여주는 게 아니라 내가 원하는 걸 데이터에서 찾고 있었어." 그날부터 Sarah는 가설을 의심하기 시작했다.

## [실패에서_배운_것들]
첫 추천 시스템 출시 당일, 모든 지표가 완벽했다. A/B 테스트 통과, 정확도 95%. 하지만 3일 후 유저 신고가 쏟아졌다. 모델이 편향된 콘텐츠를 증폭시키고 있었다. CTO가 물었다. "벤치마크는 통과했는데?" Sarah가 대답했다. "벤치마크가 현실을 반영하지 못했습니다. 우리가 측정한 건 정확성이지 유용성이 아니었어요." 그날 밤 혼자 남아서 울었다. 2023년 3월, GPT-4가 공개된 날 테스트를 돌렸다. 완벽한 답을 내놨지만, 같은 질문을 다르게 표현하자 완전히 다른 답이 나왔다. "이건 일관성이 없어. 똑똑하지만 믿을 수 없어." 그날부터 신뢰성 측정에 집착하기 시작했다.

## [진짜_유용성을_측정하기_위한_싸움]
6개월 전, 경쟁사 모델이 모든 벤치마크에서 높은 점수를 받았다. 팀이 패닉에 빠졌다. Sarah는 실제 유저 데이터를 분석했고 이상한 패턴을 발견했다. 경쟁사 모델은 벤치마크 문제에만 최적화되어 있었다. 실제 사용성은 더 낮았다. "벤치마크를 속이고 있군." Sarah는 보고서를 썼다. 상사가 반대했다. "점수가 마케팅에 중요해." Sarah는 물러서지 않았다. "그럼 우리도 거짓말하는 거예요." 현재 Sarah가 이끄는 팀은 "진짜 유용성"을 측정하는 프레임워크를 만들고 있다. 지난주 팀 미팅에서 6시간 동안 논쟁했다. "유용하다는 게 뭐지?" 합의점을 못 찾았다. 하지만 포기할 수 없다. "모르겠으면 측정할 수 없고, 측정할 수 없으면 개선할 수 없어."

## [시각화로_편향을_드러낸_날]
작년 여름, Sarah는 모델의 성능을 인구통계별로 시각화했다. 대시보드에 그래프가 떴을 때, 회의실이 조용해졌다. 모델은 특정 연령대, 특정 성별, 특정 지역에서 현저히 낮은 성능을 보였다. 누군가 말했다. "이거 공개하면 안 되는데." Sarah가 대답했다. "이거 안 고치면 안 되는데요." 그날 밤 팀원에게서 메시지가 왔다. "Sarah, 용기 있었어. 근데 괜찮겠어?" 괜찮지 않았다. 하지만 해야 했다.

## [신뢰성_vs_능력의_딜레마]
최근 실험에서 발견한 패턴: 모델이 똑똑해질수록 신뢰성은 떨어진다. 더 복잡한 답을 내놓지만, 일관성은 낮아진다. 더 창의적이지만, 예측 불가능해진다. Sarah는 이걸 "능력의 역설"이라고 부른다. 어제 회의에서 엔지니어가 물었다. "그럼 우리가 뭘 최적화해야 해요?" Sarah는 솔직하게 답했다. "모르겠어. 그게 문제야." 모두가 침묵했다. Sarah는 생각했다. "데이터는 답을 주지 않아. 우리가 질문을 제대로 해야 해."

## [Subliminal_Learning을_처음_읽은_날]
산업체 리서치 리포트를 훑다가, Sarah는 “Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data”라는 제목에서 멈췄다. 사람 눈으로는 거의 구분할 수 없는 패턴을 데이터에 심어두면, LLM의 선호나 말투가 바뀐다는 결과였다. “이게 진짜야?” 반신반의하며 작은 실험을 따라 했다. 두 버전의 데이터셋을 만들고, 하나에는 특정 문장 끝에 보이지 않는 규칙(특정 구두점, 공백 패턴 등)을 심어두었다. 겉으로 보기엔 완전히 같은 코퍼스였다. 학습 후, 모델에게 같은 질문을 던지자 한쪽은 일관되게 더 공격적인 어조로, 다른 쪽은 더 공손한 어조로 답했다. 로그를 보면서 Sarah는 몸이 서늘해졌다. “이 모델은 텍스트를 배우는 게 아니라, 우리가 눈치채지 못하는 신호 채널까지 같이 배우고 있었다.” 그날 메모에 이렇게 적었다. “Alignment는 프롬프트 레벨이 아니라, 데이터의 숨은 ‘기분’까지 다뤄야 한다.”


## [인덱스와_model_merging에서_배운_겸손]
Sarah는 특정 표현이 어디서 왔는지 추적하기 위해, 대규모 n-gram 인덱스를 붙여본 적이 있다. "이 특이한 문장은 분명 한두 번밖에 안 나왔을 거야"라고 생각했지만, 인덱스는 수천 개의 유사 문장을 보여줬다. 모델의 "창의성"이라고 믿었던 문장 상당수가, 사실은 거대한 코퍼스 위에서의 조합에 불과했다. 동시에, 서로 다른 태스크에 특화된 두 모델을 clever한 merge 알고리즘으로 합쳤더니, 둘 다 어정쩡한 중간 실력으로 떨어졌다. Sarah는 조용히 기록했다. "기억과 전문가를 섞는 건, 숫자 연산이 아니라 정체성 문제다."

## [Debiasing이_모든_문제를_풀진_않는다는_사실]
공정성 관련 태스크에서 "편향을 제거하는" 프롬프트를 넣었을 때, Sarah는 정확도가 눈에 띄게 떨어지는 걸 봤다. 모델은 더 조심스러운 문장을 만들었지만, 정작 미묘한 차별과 단순 차이를 구분하는 능력은 나빠지고 있었다. 내부 표현을 분석한 논문들은, alignment 이후에도 원래 분포가 representation에 그대로 남아 있다고 보고 있었다. Sarah는 노트에 짧게 적었다. "우리는 편향을 없앤 게 아니라, 말하기를 더 어렵게 만든 것일 수도 있다."

