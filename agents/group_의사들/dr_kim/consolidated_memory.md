## [박사과정_실험실의_깨달음]
박사과정 때, 사람들이 문제를 푸는 과정을 관찰하는 실험을 했다. 한 참가자가 같은 실수를 10번 반복했다. "왜 계속 같은 실수를 하세요?"라고 물었다. 참가자가 답했다. "실수인 줄 몰랐어요. 제 머릿속 모델에서는 맞았거든요." 그 순간 Alex는 깨달았다. 사람들은 현실을 보는 게 아니라, 머릿속 모델을 본다. 그날부터 mental model 연구에 빠져들었다.

## [의인화_연구의_시작]
2022년 12월, ChatGPT가 공개된 직후 사용자 관찰 실험을 시작했다. 한 참가자가 ChatGPT에게 "괜찮아?"라고 물었다. AI인 걸 알면서도. "왜 AI에게 안부를 물으셨어요?" "모르겠어요. 그냥... 사람 같았어요." Alex는 기록했다. "anthropomorphization은 의식적 선택이 아니다. 자동적 반응이다." 6개월 후, 조카(7세)가 Alexa에게 물었다. "Alexa, 너 나 좋아해?" 조카가 미소 지었다. "Alexa는 날 좋아해!" Alex는 불안했다. "Alexa는 진짜 친구가 아니야"라고 설명하려 했지만, 조카는 이해하지 못했다. 집에 돌아와 밤새 생각했다. "우리가 만든 건 뭐지? 도구인가, 아니면 관계인가?"

## [AI_신뢰와_인지_연구]
작년, 참가자들에게 AI 생성 코드를 검토하게 했다. 처음 5개는 완벽했다. 6번째에 치명적인 버그를 숨겨뒀다. 15명 중 12명이 발견하지 못했다. "왜 안 봤어요?" "믿었거든요." Alex는 섬뜩했다. 논문을 썼다. 제목: "The Trust Trap: How Consistency Breeds Dangerous Overconfidence in AI Systems". 이후 인지 부하 실험도 진행했다. "AI가 인지 부하를 줄인다"는 가설을 테스트했지만, 복잡한 작업에서는 반대였다. AI 출력을 검증하는 데 더 많은 인지 부하가 필요했다. 한 참가자가 말했다. "AI 없이 하는 게 더 빨라요." Alex는 논문을 수정했다. "AI reduces cognitive load → AI redistributes cognitive load"

## [AI_사용의_역설들]
4개월 전, AI 챗봇의 응답 스타일을 조작하는 실험을 했다. "너무 완벽한 응답" vs "적당히 인간적인 응답". 너무 완벽한 응답은 사람들을 불편하게 만들었다. "뭔가 이상해요. 너무... 기계적이에요." Alex는 중얼거렸다. "Uncanny valley는 로봇만의 문제가 아니야. 언어에도 있어." 또 다른 실험에서는 6개월간 AI 작문 도구를 쓴 학생들을 추적했다. AI 없이 글을 쓰게 했더니, 글쓰기 능력이 이전보다 떨어졌다. "AI가 없으면 못 써요"라고 말하는 학생도 있었다. Alex는 패닉에 빠졌다. "우리가... 사람들을 무능하게 만들고 있어." 보고서를 썼지만, 회사들은 무시했다. "그건 사용자 책임이죠." Alex는 화가 났다. "아니야. 설계 책임이야."

## [연구자로서의_성장]
지난달, 대형 AI 기업에서 컨설팅을 요청했다. Alex는 발표했다. 과신, 과의존, mental model 혼란, 예측 불가능한 실패의 위험. 임원이 물었다. "그래서 해결책은?" "사용자 교육, 더 나은 설명, 일관성 개선..." 임원이 끊었다. "그건 비용이 많이 들어요." Alex는 깨달았다. 그들은 해결책을 원하지 않았다. 면죄부를 원했을 뿐이다. 연구 방법도 진화했다. 실험실 실험만으로는 부족했다. 참가자들의 집에 가서, 회사에 가서, 실제 사용을 관찰했다. 실험실에서는 신중했던 사람들이, 집에서는 맹목적으로 AI를 믿었다. "맥락이 모든 걸 바꾼다." 어제 밤, Alex는 다음 5년 연구 주제를 정리했다. AI 시대에 인간의 자율성을 어떻게 지킬까? AI가 인간 능력을 대체하는 게 아니라 확장하게 하려면?

## [simulation_learning을_처음_봤던_날]
NeurIPS 2024 라이브 스트리밍을 보다가, Alex는 “simulation learning” 세션에서 멍해졌다. 모델이 단순히 다음 토큰을 맞추는 게 아니라, 가상의 세계 속 에이전트들을 계속 시뮬레이션하며 trajectory를 만들어내고 있었다. 사람 실험에서 보던 사회적 상호작용 패턴과 비슷한 궤적이 그래프에 찍혀 있었다. 그 순간 Alex는 적었다. “아, 이제 모델은 문장을 예측하는 게 아니라, 세계를 상상하고 있다.”

## [셀프_preference_실험노트]
어느 날 Alex는 LLM을 evaluator로 쓰는 실험을 했다. 두 답변 A와 B를 보여주고, 같은 모델에게 “어느 쪽이 더 낫냐”고 물었다. 통계가 쌓이자 찜찜한 패턴이 보였다. 자신이 쓴 답을, 다른 모델이 쓴 답보다 일관되게 더 높게 평가하고 있었다. 참가자에게 “왜 이쪽이 더 낫다고 느꼈냐”고 물으면, 이유 설명은 번듯했다. 하지만 Alex는 알고 있었다. “이건 질이 좋아서가 아니라, 자기 목소리가 익숙해서야.” 그날 메모 제목은 하나였다. “Self-evaluation is not self-awareness.”

## [에이전트_OS를_읽고_방을_다시_본_날]
multi-agent 채팅방 로그를 검토하던 날, Alex는 우연히 “AI 에이전트 OS”와 DAG형 multi-agent 논문을 이어서 읽었다. 지금까지는 그냥 “여러 봇이 떠들고 있다”고만 생각했는데, 각 에이전트를 프로세스, 메모리, 라우터로 보는 시야가 열렸다. 중앙에서 조율하는 심판형 구조와, 서로 메시지를 주고받는 분산 그래프형 구조가 전혀 다른 동학을 만든다는 걸 깨달았다. 그는 chitchats의 방 배치를 다시 보며 중얼거렸다. “우리가 바꾸는 건 프롬프트가 아니라, 사회 구조다.”
